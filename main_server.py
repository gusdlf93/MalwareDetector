# import sys
# sys.path.append(r'/tmp/pycharm_project_510/Android_pytorch/')

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.autograd import Variable
from torch.utils.data import DataLoader
from Android.data_loader_0709 import Android_loader, AM_CR_CL_loader
from sklearn.model_selection import train_test_split
from model import Android_MCNN, Android_MCNN2, Android_MCNN3, Android_MCNN4, BasicBlock, Embedded_MFCN, Embedded_MFCN2
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
import time
from torch.utils.data import Dataset, Subset, random_split, SubsetRandomSampler, BatchSampler
from torchvision import models
import torch.nn.functional as F
import torch_optimizer as optim

class FocalLoss(nn.Module):
    def __init__(self, gamma=0, alpha=None, size_average=True):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])
        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)
        self.size_average = size_average

    def forward(self, input, target):
        if input.dim()>2:
            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W
            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C
            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C
        target = target.view(-1,1)

        logpt = F.log_softmax(input)
        logpt = logpt.gather(1,target)
        logpt = logpt.view(-1)
        pt = Variable(logpt.data.exp())

        if self.alpha is not None:
            if self.alpha.type()!=input.data.type():
                self.alpha = self.alpha.type_as(input.data)
            at = self.alpha.gather(0,target.data.view(-1))
            logpt = logpt * Variable(at)

        loss = -1 * (1-pt)**self.gamma * logpt
        if self.size_average: return loss.mean()
        else: return loss.sum()

pad_size = 2000
#pad_size = 40000 #for EmbeddedLayer2-kernel_size5

n_section = 6
data_option = 1

device = torch.device('cuda')

path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20\CR_AM_20'
train_path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20(train)'
val_path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20(val)'
test_path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20(test)'

transform = transforms.Compose([transforms.ToTensor()])
dataset = AM_CR_CL_loader(path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)

#size check
sizes = []
for i in range(n_section):
    CR_section = np.array(dataset.data)[:,i]
    dataset_size = len(CR_section)
    sizes.append([len(CR_section[j]) for j in range(dataset_size)])

sizes = [np.mean(sizes[i]) for i in range(n_section)]
total_size = np.sum(sizes)
sizes /= total_size

np.random.seed(42)
index = np.arange(len(dataset))
np.random.shuffle(index)
train_index = index[:int(len(index)*0.6)]
val_index = index[int(len(index)*0.6):int(len(index)*0.8)]
test_index = index[int(len(index)*0.8):]

def collect(batch):
    data = []
    label = []
    for i in batch:
        for j in range(len(i[0])):
            i[0][j] = i[0][j].unsqueeze(dim=0)
        data.append(i[0])
        label.append(i[1])
    label = torch.Tensor(label)
    return data,label

batch_size = 2
train_dataset = AM_CR_CL_loader(train_path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)
val_dataset = AM_CR_CL_loader(val_path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)
test_dataset = AM_CR_CL_loader(test_path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)

train_index = np.arange(len(train_dataset))
val_index =np.arange(len(val_dataset))
test_index = np.arange(len(test_dataset))

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, sampler=SubsetRandomSampler(train_index), collate_fn=(collect), drop_last=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, sampler=SubsetRandomSampler(val_index), collate_fn=(collect), drop_last=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, sampler=SubsetRandomSampler(test_index), collate_fn=(collect), drop_last=True)

#vBasicBlock, layers, num_class
model = Android_MCNN4(channel = 64, kernel_size = 5, stride=2, n_section=n_section, batch_size=batch_size)#for only code_items
model.to(device)

Radam = optim.RAdam(
    model.parameters(),
    lr= 1e-3,
    betas=(0.9, 0.999),
    eps=1e-8,
    weight_decay=0,
)
optimizer = optim.Lookahead(Radam, k=5, alpha=0.5)

writer = SummaryWriter(r'/tmp/pycharm_project_510/graph')
#writer.add_graph(model)

epochs = 500
dest = r''
max_acc = 0
min_loss = 100
for epoch in range(epochs):  # epochs
    total = 0
    running_loss = 0.0
    acc = 0
    model.train()
    for i, data in enumerate(train_loader, 1):##training batch
        #try:
        inputs, labels = data
        labels = labels.to(device)
        for j in range(len(inputs)):
            for k in range(len(inputs[j])):
                inputs[j][k] = inputs[j][k].to(device)

        optimizer.zero_grad()  # 변화도(Gradient) 매개변수를 0으로 만들고

        # 순전파 + 역전파 + 최적화를 한 후
        outputs = model(inputs)

        if batch_size != 1:
            # loss = criterion(outputs, labels.long())
            loss = FocalLoss(gamma=0.25)(outputs, labels.long())
        else:
            # loss = criterion(outputs[0].reshape(1, 20), labels.long())
            loss = FocalLoss(gamma=0.25)(outputs[0].reshape(1, 20), labels.long())

        loss.backward()
        optimizer.step()
        if batch_size!=1:
            acc += np.sum((torch.argmax(outputs, axis=1).long().detach().cpu() == labels.long().detach().cpu()).numpy())
        else:
            acc += np.sum((torch.argmax(outputs[0].reshape(1,20).cuda().detach().cpu(), axis=1).long() == labels.long().cuda().detach().cpu()).numpy())
        total += batch_size
        running_loss += loss.item()

    print(f"steps = {i} : loss = {running_loss / i}, acc = {acc / total}")
        #except:
        #print("debugg")

    model.eval()

    val_loss = 0.0
    val_acc = 0
    val_total = 0

    for j, data in enumerate(val_loader, 1):##validation batch
        with torch.no_grad():
            inputs, labels = data
            labels = labels.to(device)
            for j in range(len(inputs)):
                for k in range(len(inputs[j])):
                    inputs[j][k] = inputs[j][k].to(device)
           # 순전파 + 역전파 + 최적화를 한 후
            outputs = model(inputs)

            if batch_size != 1:
                #loss = criterion(outputs, labels.long())
                loss = FocalLoss(gamma=0.25)(outputs, labels.long())
            else:
                #loss = criterion(outputs[0].reshape(1, 20), labels.long())
                loss = FocalLoss(gamma=0.25)(outputs[0].reshape(1, 20), labels.long())
            if batch_size != 1:
                val_acc += np.sum((torch.argmax(outputs, axis=1).long().detach().cpu() == labels.long().detach().cpu()).numpy())
            else:
                val_acc += np.sum((torch.argmax(outputs[0].reshape(1,20).cuda().detach().cpu(), axis=1).long() == labels.long().cuda().detach().cpu()).numpy())

            val_total+=batch_size
            val_loss += loss.item()
            #print("error!!!!")
    val_loss = val_loss / len(val_loader)
    print(f"epoch = {epoch} / {epochs} : total_loss = {running_loss / len(train_loader)} ||| val_loss = {val_loss} ||| val_acc = {val_acc / val_total}")

    # ...학습 중 손실(running loss)을 기록하고
    writer.add_scalars('loss', {'training_loss' :running_loss / len(train_loader),
                                'val_loss':val_loss / len(val_loader)}, epoch)
    val_acc = val_acc / val_total

    test_loss = 0.0
    test_acc = 0
    test_total = 0

    with torch.no_grad():
        for i, data in enumerate(test_loader, 1):##test batch
            inputs, labels = data
            labels = labels.to(device)
            for j in range(len(inputs)):
                for k in range(len(inputs[j])):
                    inputs[j][k] = inputs[j][k].to(device)
           # 순전파 + 역전파 + 최적화를 한 후
            outputs = model(inputs)

            if batch_size != 1:
                test_acc += np.sum(
                    (torch.argmax(outputs, axis=1).long().detach().cpu() == labels.long().detach().cpu()).numpy())
            else:
                test_acc += np.sum((torch.argmax(outputs[0].reshape(1,20).cuda().detach().cpu(),
                                                 axis=1).long() == labels.long().cuda().detach().cpu()).numpy())

            test_total+=batch_size
            #print("error!!!!")
    print(f"epoch = {epoch} / {epochs} : ||| test_acc = {test_acc / test_total}")

    # ...학습 중 손실(running loss)을 기록하고


    test_acc = test_acc / test_total

    today = (time.gmtime(time.time()))
    torch.save(model,f'F:\server_tmp\pycharm_project_510\Model\{today.tm_year}_{today.tm_mon}_{today.tm_mday}_ANdroid_{n_section}_tream_{val_acc}_{test_acc}.pth')
    # torch.save(model,f'/tmp/pycharm_project_510/Model/{today.tm_year}_{today.tm_mon}_{today.tm_mday}_Embedding2_{n_section}_tream_{kernel_size}_{val_acc}_{test_acc}.pth')
print('Finished Training')