import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torchvision import transforms
from torch.utils.data import Dataset, Subset, random_split, SubsetRandomSampler, BatchSampler
from torchvision import models
from torch.utils.tensorboard import SummaryWriter

import numpy as np
from sklearn.model_selection import train_test_split
from model import Android_MCNN4
from data_loader import Android_loader, AM_CR_CL_loader
import time
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sklearn import svm, metrics

device = torch.device('cuda')

pad_size = 2000
n_section = 6
data_option = 1

transform = transforms.Compose([transforms.ToTensor()])

train_path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20(train)'
val_path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20(val)'
test_path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20(test)'

train_dataset = AM_CR_CL_loader(train_path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)
val_dataset = AM_CR_CL_loader(val_path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)
test_dataset = AM_CR_CL_loader(test_path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)

train_index = np.arange(len(train_dataset))
val_index =np.arange(len(val_dataset))
test_index = np.arange(len(test_dataset))

def collect(batch):
    data = []
    label = []
    for i in batch:
        for j in range(len(i[0])):
            i[0][j] = i[0][j].unsqueeze(dim=0)
        data.append(i[0])
        label.append(i[1])
    label = torch.Tensor(label)
    return data,label

batch_size = 2

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, sampler=SubsetRandomSampler(train_index), collate_fn=(collect), drop_last=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, sampler=SubsetRandomSampler(val_index), collate_fn=(collect), drop_last=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, sampler=SubsetRandomSampler(test_index), collate_fn=(collect), drop_last=True)

model_path = r'C:\Users\User\Documents\GitHub\Malware-Detector\MalwareDetector\MD_weights.pth'

model = torch.load(model_path)
model.eval()
model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())

acc = 0
total = 0
test_loss = 0
Y_Pred = []
Y_Test = []

with torch.no_grad():
    for i, data in enumerate(test_loader, 1):  ##training batch
        print(i)
        inputs, labels = data
        labels = labels.to(device)
        for j in range(len(inputs)):
            for k in range(len(inputs[j])):
                inputs[j][k] = inputs[j][k].to(device)
        Y_Test.append(labels.detach().cpu().numpy())

        outputs = model(inputs)
        Y_Pred.append(outputs.detach().cpu().numpy())

        if batch_size != 1:
            acc += np.sum(
                (torch.argmax(outputs, axis=1).long().detach().cpu() == labels.long().detach().cpu()).numpy())
        else:
            acc += np.sum((torch.argmax(outputs[0].reshape(1, 20).cuda().detach().cpu(),
                                        axis=1).long() == labels.long().cuda().detach().cpu()).numpy())

        total += batch_size

print(f"acc = {acc / total}")
Y_Test_class = np.asarray(Y_Test).reshape(len(Y_Test) * len(Y_Test[0]))
Y_Pred_class = np.argmax(Y_Pred, axis=2).reshape(len(Y_Pred) * len(Y_Pred[0]))

print(classification_report(Y_Test_class, Y_Pred_class, digits=3))
print(confusion_matrix(Y_Test_class, Y_Pred_class))