import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import transforms
from torch.utils.data import Dataset, Subset, random_split, SubsetRandomSampler, BatchSampler
from torchvision import models
import torch.nn.functional as F
import torch_optimizer as optim

import numpy as np
from data_loader import Android_loader, AM_CR_CL_loader
from sklearn.model_selection import train_test_split
from model import Android_MCNN, Android_MCNN2, Android_MCNN3, Android_MCNN4, BasicBlock, Embedded_MFCN, Embedded_MFCN2

import time


class FocalLoss(nn.Module):
    def __init__(self, gamma=0, alpha=None, size_average=True):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])
        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)
        self.size_average = size_average

    def forward(self, input, target):
        if input.dim()>2:
            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W
            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C
            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C
        target = target.view(-1,1)

        logpt = F.log_softmax(input)
        logpt = logpt.gather(1,target)
        logpt = logpt.view(-1)
        pt = Variable(logpt.data.exp())

        if self.alpha is not None:
            if self.alpha.type()!=input.data.type():
                self.alpha = self.alpha.type_as(input.data)
            at = self.alpha.gather(0,target.data.view(-1))
            logpt = logpt * Variable(at)

        loss = -1 * (1-pt)**self.gamma * logpt
        if self.size_average: return loss.mean()
        else: return loss.sum()

pad_size = 2000
#pad_size = 40000 #for EmbeddedLayer2-kernel_size5

n_section = 6
data_option = 1

device = torch.device('cuda')

path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20\CR_AM_20'
train_path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20(train)'
val_path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20(val)'
test_path = r'F:\server_tmp\pycharm_project_803\Data\CR_AM_20(test)'

transform = transforms.Compose([transforms.ToTensor()])
dataset = AM_CR_CL_loader(path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)

#size check
sizes = []
for i in range(n_section):
    CR_section = np.array(dataset.data)[:,i]
    dataset_size = len(CR_section)
    sizes.append([len(CR_section[j]) for j in range(dataset_size)])

sizes = [np.mean(sizes[i]) for i in range(n_section)]
total_size = np.sum(sizes)
sizes /= total_size

np.random.seed(42)
index = np.arange(len(dataset))
np.random.shuffle(index)
train_index = index[:int(len(index)*0.6)]
val_index = index[int(len(index)*0.6):int(len(index)*0.8)]
test_index = index[int(len(index)*0.8):]

def collect(batch):
    data = []
    label = []
    for i in batch:
        for j in range(len(i[0])):
            i[0][j] = i[0][j].unsqueeze(dim=0)
        data.append(i[0])
        label.append(i[1])
    label = torch.Tensor(label)
    return data,label

batch_size = 2
train_dataset = AM_CR_CL_loader(train_path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)
val_dataset = AM_CR_CL_loader(val_path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)
test_dataset = AM_CR_CL_loader(test_path, transform=transform, pad_size = pad_size, n_section=n_section, data_option=data_option)

train_index = np.arange(len(train_dataset))
val_index =np.arange(len(val_dataset))
test_index = np.arange(len(test_dataset))

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, sampler=SubsetRandomSampler(train_index), collate_fn=(collect), drop_last=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size, sampler=SubsetRandomSampler(val_index), collate_fn=(collect), drop_last=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, sampler=SubsetRandomSampler(test_index), collate_fn=(collect), drop_last=True)

#vBasicBlock, layers, num_class
model = Android_MCNN4(channel = 64, kernel_size = 5, stride=2, n_section=n_section, batch_size=batch_size)#for only code_items
model.to(device)

Radam = optim.RAdam(
    model.parameters(),
    lr= 1e-3,
    betas=(0.9, 0.999),
    eps=1e-8,
    weight_decay=0,
)
optimizer = optim.Lookahead(Radam, k=5, alpha=0.5)

writer = SummaryWriter(r'/tmp/pycharm_project_510/graph')
#writer.add_graph(model)

epochs = 500
dest = r''
max_acc = 0
min_loss = 100
for epoch in range(epochs):  # epochs
    total = 0
    running_loss = 0.0
    acc = 0
    model.train()
    for i, data in enumerate(train_loader, 1):##training batch
        #try:
        inputs, labels = data
        labels = labels.to(device)
        for j in range(len(inputs)):
            for k in range(len(inputs[j])):
                inputs[j][k] = inputs[j][k].to(device)

        optimizer.zero_grad()  # 변화도(Gradient) 매개변수를 0으로 만들고

        # 순전파 + 역전파 + 최적화를 한 후
        outputs = model(inputs)

        if batch_size != 1:
            # loss = criterion(outputs, labels.long())
            loss = FocalLoss(gamma=0.25)(outputs, labels.long())
        else:
            # loss = criterion(outputs[0].reshape(1, 20), labels.long())
            loss = FocalLoss(gamma=0.25)(outputs[0].reshape(1, 20), labels.long())

        loss.backward()
        optimizer.step()
        if batch_size!=1:
            acc += np.sum((torch.argmax(outputs, axis=1).long().detach().cpu() == labels.long().detach().cpu()).numpy())
        else:
            acc += np.sum((torch.argmax(outputs[0].reshape(1,20).cuda().detach().cpu(), axis=1).long() == labels.long().cuda().detach().cpu()).numpy())
        total += batch_size
        running_loss += loss.item()

    print(f"steps = {i} : loss = {running_loss / i}, acc = {acc / total}")
        #except:
        #print("debugg")

    model.eval()

    val_loss = 0.0
    val_acc = 0
    val_total = 0

    for j, data in enumerate(val_loader, 1):##validation batch
        with torch.no_grad():
            inputs, labels = data
            labels = labels.to(device)
            for j in range(len(inputs)):
                for k in range(len(inputs[j])):
                    inputs[j][k] = inputs[j][k].to(device)
           # 순전파 + 역전파 + 최적화를 한 후
            outputs = model(inputs)

            if batch_size != 1:
                #loss = criterion(outputs, labels.long())
                loss = FocalLoss(gamma=0.25)(outputs, labels.long())
            else:
                #loss = criterion(outputs[0].reshape(1, 20), labels.long())
                loss = FocalLoss(gamma=0.25)(outputs[0].reshape(1, 20), labels.long())
            if batch_size != 1:
                val_acc += np.sum((torch.argmax(outputs, axis=1).long().detach().cpu() == labels.long().detach().cpu()).numpy())
            else:
                val_acc += np.sum((torch.argmax(outputs[0].reshape(1,20).cuda().detach().cpu(), axis=1).long() == labels.long().cuda().detach().cpu()).numpy())

            val_total+=batch_size
            val_loss += loss.item()
            #print("error!!!!")
    val_loss = val_loss / len(val_loader)
    print(f"epoch = {epoch} / {epochs} : total_loss = {running_loss / len(train_loader)} ||| val_loss = {val_loss} ||| val_acc = {val_acc / val_total}")

    # ...학습 중 손실(running loss)을 기록하고
    writer.add_scalars('loss', {'training_loss' :running_loss / len(train_loader),
                                'val_loss':val_loss / len(val_loader)}, epoch)
    val_acc = val_acc / val_total

    test_loss = 0.0
    test_acc = 0
    test_total = 0

    with torch.no_grad():
        for i, data in enumerate(test_loader, 1):##test batch
            inputs, labels = data
            labels = labels.to(device)
            for j in range(len(inputs)):
                for k in range(len(inputs[j])):
                    inputs[j][k] = inputs[j][k].to(device)
           # 순전파 + 역전파 + 최적화를 한 후
            outputs = model(inputs)

            if batch_size != 1:
                test_acc += np.sum(
                    (torch.argmax(outputs, axis=1).long().detach().cpu() == labels.long().detach().cpu()).numpy())
            else:
                test_acc += np.sum((torch.argmax(outputs[0].reshape(1,20).cuda().detach().cpu(),
                                                 axis=1).long() == labels.long().cuda().detach().cpu()).numpy())

            test_total+=batch_size
            #print("error!!!!")
    print(f"epoch = {epoch} / {epochs} : ||| test_acc = {test_acc / test_total}")

    # ...학습 중 손실(running loss)을 기록하고


    test_acc = test_acc / test_total

    today = (time.gmtime(time.time()))
    torch.save(model,f'F:\server_tmp\pycharm_project_510\Model\{today.tm_year}_{today.tm_mon}_{today.tm_mday}_ANdroid_{n_section}_tream_{val_acc}_{test_acc}.pth')
    # torch.save(model,f'/tmp/pycharm_project_510/Model/{today.tm_year}_{today.tm_mon}_{today.tm_mday}_Embedding2_{n_section}_tream_{kernel_size}_{val_acc}_{test_acc}.pth')
print('Finished Training')